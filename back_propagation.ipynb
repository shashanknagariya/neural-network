{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMMQAnyn4frIriSsjGMzDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashanknagariya/neural-network/blob/main/back_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Coding back propagation on a single neuron from scratch**"
      ],
      "metadata": {
        "id": "rRkDQoVofco0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "02p9wPzleBBP"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate the weights and inputs\n",
        "weights = np.array([0.5,0.8,1])\n",
        "inputs = np.array([-1,2,-1])\n",
        "bias = 0.1\n",
        "target_output = 0.0\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "AKc9PKqlfx8a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(x,0)"
      ],
      "metadata": {
        "id": "8LjgcwNfgp7C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_derivate(x):\n",
        "  return np.where(x>0, 1.0, 0.0)"
      ],
      "metadata": {
        "id": "9KyPkzExg3wf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range(200):\n",
        "  #forward pass\n",
        "  linear_output = np.dot(weights, inputs) + bias\n",
        "  output = relu(linear_output)\n",
        "  loss = (output-target_output)**2\n",
        "\n",
        "  #backward pass\n",
        "  dloss_doutput = 2*(output-target_output)\n",
        "  doutput_dlinear = relu_derivate(linear_output)\n",
        "  dlinear_dweights = inputs\n",
        "  dlinear_dbias = 1\n",
        "\n",
        "  dloss_dweights = dloss_doutput * doutput_dlinear * dlinear_dweights\n",
        "  dloss_dbias = dloss_doutput * doutput_dlinear * dlinear_dbias\n",
        "\n",
        "  #update weights\n",
        "  weights -= learning_rate * dloss_dweights\n",
        "  bias -= learning_rate * dloss_dbias\n",
        "\n",
        "  # Print the loss for this iteration\n",
        "  print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
        "print(\"Final weights:\", weights)\n",
        "print(\"Final bias:\", bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYQAPDGphBZo",
        "outputId": "1cbf03ab-898b-477b-828c-a5c220716ed9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 0.00014217589401377113\n",
            "Iteration 2, Loss: 0.00013822283545661388\n",
            "Iteration 3, Loss: 0.00013437968773957656\n",
            "Iteration 4, Loss: 0.00013064339490166465\n",
            "Iteration 5, Loss: 0.0001270109859498189\n",
            "Iteration 6, Loss: 0.00012347957249647217\n",
            "Iteration 7, Loss: 0.00012004634646278009\n",
            "Iteration 8, Loss: 0.00011670857784572664\n",
            "Iteration 9, Loss: 0.00011346361254730316\n",
            "Iteration 10, Loss: 0.00011030887026403945\n",
            "Iteration 11, Loss: 0.00010724184243522112\n",
            "Iteration 12, Loss: 0.0001042600902481513\n",
            "Iteration 13, Loss: 0.00010136124269889021\n",
            "Iteration 14, Loss: 9.854299470689046e-05\n",
            "Iteration 15, Loss: 9.580310528205772e-05\n",
            "Iteration 16, Loss: 9.313939574279371e-05\n",
            "Iteration 17, Loss: 9.054974798356117e-05\n",
            "Iteration 18, Loss: 8.803210279062768e-05\n",
            "Iteration 19, Loss: 8.55844582046383e-05\n",
            "Iteration 20, Loss: 8.32048679287193e-05\n",
            "Iteration 21, Loss: 8.089143978082726e-05\n",
            "Iteration 22, Loss: 7.864233418915943e-05\n",
            "Iteration 23, Loss: 7.645576272936365e-05\n",
            "Iteration 24, Loss: 7.432998670243357e-05\n",
            "Iteration 25, Loss: 7.226331575215937e-05\n",
            "Iteration 26, Loss: 7.025410652098574e-05\n",
            "Iteration 27, Loss: 6.83007613432772e-05\n",
            "Iteration 28, Loss: 6.64017269748882e-05\n",
            "Iteration 29, Loss: 6.455549335807875e-05\n",
            "Iteration 30, Loss: 6.276059242075052e-05\n",
            "Iteration 31, Loss: 6.1015596909084184e-05\n",
            "Iteration 32, Loss: 5.931911925262394e-05\n",
            "Iteration 33, Loss: 5.766981046092259e-05\n",
            "Iteration 34, Loss: 5.606635905086779e-05\n",
            "Iteration 35, Loss: 5.450749000381701e-05\n",
            "Iteration 36, Loss: 5.299196375175069e-05\n",
            "Iteration 37, Loss: 5.151857519159765e-05\n",
            "Iteration 38, Loss: 5.008615272696966e-05\n",
            "Iteration 39, Loss: 4.869355733655072e-05\n",
            "Iteration 40, Loss: 4.7339681668364024e-05\n",
            "Iteration 41, Loss: 4.602344915925672e-05\n",
            "Iteration 42, Loss: 4.474381317883396e-05\n",
            "Iteration 43, Loss: 4.349975619721094e-05\n",
            "Iteration 44, Loss: 4.229028897590425e-05\n",
            "Iteration 45, Loss: 4.11144497812182e-05\n",
            "Iteration 46, Loss: 3.997130361950135e-05\n",
            "Iteration 47, Loss: 3.8859941493664906e-05\n",
            "Iteration 48, Loss: 3.777947968037404e-05\n",
            "Iteration 49, Loss: 3.6729059027339515e-05\n",
            "Iteration 50, Loss: 3.570784427014273e-05\n",
            "Iteration 51, Loss: 3.4715023368056495e-05\n",
            "Iteration 52, Loss: 3.3749806858331496e-05\n",
            "Iteration 53, Loss: 3.281142722844255e-05\n",
            "Iteration 54, Loss: 3.189913830578345e-05\n",
            "Iteration 55, Loss: 3.10122146643302e-05\n",
            "Iteration 56, Loss: 3.0149951047804022e-05\n",
            "Iteration 57, Loss: 2.9311661808870334e-05\n",
            "Iteration 58, Loss: 2.849668036393641e-05\n",
            "Iteration 59, Loss: 2.7704358663097202e-05\n",
            "Iteration 60, Loss: 2.6934066674829598e-05\n",
            "Iteration 61, Loss: 2.618519188500283e-05\n",
            "Iteration 62, Loss: 2.5457138809832586e-05\n",
            "Iteration 63, Loss: 2.4749328522365026e-05\n",
            "Iteration 64, Loss: 2.406119819212905e-05\n",
            "Iteration 65, Loss: 2.339220063759438e-05\n",
            "Iteration 66, Loss: 2.27418038910672e-05\n",
            "Iteration 67, Loss: 2.210949077567897e-05\n",
            "Iteration 68, Loss: 2.1494758494151237e-05\n",
            "Iteration 69, Loss: 2.0897118228980537e-05\n",
            "Iteration 70, Loss: 2.0316094753741518e-05\n",
            "Iteration 71, Loss: 1.9751226055208905e-05\n",
            "Iteration 72, Loss: 1.9202062965969034e-05\n",
            "Iteration 73, Loss: 1.8668168807263413e-05\n",
            "Iteration 74, Loss: 1.8149119041744857e-05\n",
            "Iteration 75, Loss: 1.7644500935907886e-05\n",
            "Iteration 76, Loss: 1.7153913231885556e-05\n",
            "Iteration 77, Loss: 1.6676965828386734e-05\n",
            "Iteration 78, Loss: 1.6213279470494722e-05\n",
            "Iteration 79, Loss: 1.576248544809643e-05\n",
            "Iteration 80, Loss: 1.532422530269799e-05\n",
            "Iteration 81, Loss: 1.489815054238228e-05\n",
            "Iteration 82, Loss: 1.448392236470221e-05\n",
            "Iteration 83, Loss: 1.4081211387274156e-05\n",
            "Iteration 84, Loss: 1.3689697385861716e-05\n",
            "Iteration 85, Loss: 1.3309069039744048e-05\n",
            "Iteration 86, Loss: 1.2939023684163017e-05\n",
            "Iteration 87, Loss: 1.257926706964933e-05\n",
            "Iteration 88, Loss: 1.222951312804482e-05\n",
            "Iteration 89, Loss: 1.1889483745032517e-05\n",
            "Iteration 90, Loss: 1.1558908538986626e-05\n",
            "Iteration 91, Loss: 1.1237524645968155e-05\n",
            "Iteration 92, Loss: 1.0925076510711274e-05\n",
            "Iteration 93, Loss: 1.0621315683407522e-05\n",
            "Iteration 94, Loss: 1.0326000622145467e-05\n",
            "Iteration 95, Loss: 1.0038896500847592e-05\n",
            "Iteration 96, Loss: 9.759775022537833e-06\n",
            "Iteration 97, Loss: 9.488414237811944e-06\n",
            "Iteration 98, Loss: 9.224598368343911e-06\n",
            "Iteration 99, Loss: 8.968117635310933e-06\n",
            "Iteration 100, Loss: 8.718768092577887e-06\n",
            "Iteration 101, Loss: 8.476351464531037e-06\n",
            "Iteration 102, Loss: 8.24067498841114e-06\n",
            "Iteration 103, Loss: 8.011551261033573e-06\n",
            "Iteration 104, Loss: 7.788798089771828e-06\n",
            "Iteration 105, Loss: 7.572238347684696e-06\n",
            "Iteration 106, Loss: 7.361699832665581e-06\n",
            "Iteration 107, Loss: 7.157015130518937e-06\n",
            "Iteration 108, Loss: 6.958021481829846e-06\n",
            "Iteration 109, Loss: 6.764560652548651e-06\n",
            "Iteration 110, Loss: 6.57647880816517e-06\n",
            "Iteration 111, Loss: 6.39362639138299e-06\n",
            "Iteration 112, Loss: 6.21585800319646e-06\n",
            "Iteration 113, Loss: 6.043032287275868e-06\n",
            "Iteration 114, Loss: 5.875011817560496e-06\n",
            "Iteration 115, Loss: 5.711662988985045e-06\n",
            "Iteration 116, Loss: 5.5528559112390976e-06\n",
            "Iteration 117, Loss: 5.398464305482785e-06\n",
            "Iteration 118, Loss: 5.2483654039324295e-06\n",
            "Iteration 119, Loss: 5.1024398522415075e-06\n",
            "Iteration 120, Loss: 4.960571614589185e-06\n",
            "Iteration 121, Loss: 4.822647881417105e-06\n",
            "Iteration 122, Loss: 4.688558979721477e-06\n",
            "Iteration 123, Loss: 4.558198285849244e-06\n",
            "Iteration 124, Loss: 4.431462140709467e-06\n",
            "Iteration 125, Loss: 4.3082497673488525e-06\n",
            "Iteration 126, Loss: 4.188463190817152e-06\n",
            "Iteration 127, Loss: 4.072007160259738e-06\n",
            "Iteration 128, Loss: 3.95878907317548e-06\n",
            "Iteration 129, Loss: 3.84871890178463e-06\n",
            "Iteration 130, Loss: 3.74170912143921e-06\n",
            "Iteration 131, Loss: 3.637674641027257e-06\n",
            "Iteration 132, Loss: 3.5365327353081584e-06\n",
            "Iteration 133, Loss: 3.438202979136023e-06\n",
            "Iteration 134, Loss: 3.342607183504094e-06\n",
            "Iteration 135, Loss: 3.2496693333742747e-06\n",
            "Iteration 136, Loss: 3.1593155272291725e-06\n",
            "Iteration 137, Loss: 3.0714739183104124e-06\n",
            "Iteration 138, Loss: 2.986074657485701e-06\n",
            "Iteration 139, Loss: 2.903049837709138e-06\n",
            "Iteration 140, Loss: 2.822333440021803e-06\n",
            "Iteration 141, Loss: 2.743861281054979e-06\n",
            "Iteration 142, Loss: 2.667570961996584e-06\n",
            "Iteration 143, Loss: 2.59340181896953e-06\n",
            "Iteration 144, Loss: 2.521294874795237e-06\n",
            "Iteration 145, Loss: 2.451192792096209e-06\n",
            "Iteration 146, Loss: 2.383039827704517e-06\n",
            "Iteration 147, Loss: 2.316781788335095e-06\n",
            "Iteration 148, Loss: 2.252365987492181e-06\n",
            "Iteration 149, Loss: 2.1897412035755394e-06\n",
            "Iteration 150, Loss: 2.128857639151182e-06\n",
            "Iteration 151, Loss: 2.069666881352429e-06\n",
            "Iteration 152, Loss: 2.012121863383221e-06\n",
            "Iteration 153, Loss: 1.956176827093669e-06\n",
            "Iteration 154, Loss: 1.9017872865934143e-06\n",
            "Iteration 155, Loss: 1.8489099928774169e-06\n",
            "Iteration 156, Loss: 1.7975028994355865e-06\n",
            "Iteration 157, Loss: 1.747525128819424e-06\n",
            "Iteration 158, Loss: 1.6989369401375998e-06\n",
            "Iteration 159, Loss: 1.6516996974539606e-06\n",
            "Iteration 160, Loss: 1.6057758390659429e-06\n",
            "Iteration 161, Loss: 1.5611288476363817e-06\n",
            "Iteration 162, Loss: 1.5177232211566636e-06\n",
            "Iteration 163, Loss: 1.4755244447155519e-06\n",
            "Iteration 164, Loss: 1.434498963054366e-06\n",
            "Iteration 165, Loss: 1.394614153885462e-06\n",
            "Iteration 166, Loss: 1.3558383019511258e-06\n",
            "Iteration 167, Loss: 1.3181405738035215e-06\n",
            "Iteration 168, Loss: 1.2814909932892638e-06\n",
            "Iteration 169, Loss: 1.245860417711826e-06\n",
            "Iteration 170, Loss: 1.2112205146577874e-06\n",
            "Iteration 171, Loss: 1.177543739468482e-06\n",
            "Iteration 172, Loss: 1.1448033133365083e-06\n",
            "Iteration 173, Loss: 1.1129732020124287e-06\n",
            "Iteration 174, Loss: 1.082028095103652e-06\n",
            "Iteration 175, Loss: 1.0519433859473894e-06\n",
            "Iteration 176, Loss: 1.0226951520444772e-06\n",
            "Iteration 177, Loss: 9.942601360371242e-07\n",
            "Iteration 178, Loss: 9.666157272145439e-07\n",
            "Iteration 179, Loss: 9.397399435349081e-07\n",
            "Iteration 180, Loss: 9.136114141447189e-07\n",
            "Iteration 181, Loss: 8.882093623859338e-07\n",
            "Iteration 182, Loss: 8.635135892740665e-07\n",
            "Iteration 183, Loss: 8.395044574380002e-07\n",
            "Iteration 184, Loss: 8.161628755033137e-07\n",
            "Iteration 185, Loss: 7.934702829129921e-07\n",
            "Iteration 186, Loss: 7.714086351668128e-07\n",
            "Iteration 187, Loss: 7.499603894747587e-07\n",
            "Iteration 188, Loss: 7.291084908058751e-07\n",
            "Iteration 189, Loss: 7.088363583273763e-07\n",
            "Iteration 190, Loss: 6.891278722205234e-07\n",
            "Iteration 191, Loss: 6.699673608613361e-07\n",
            "Iteration 192, Loss: 6.513395883600064e-07\n",
            "Iteration 193, Loss: 6.332297424453226e-07\n",
            "Iteration 194, Loss: 6.156234226865377e-07\n",
            "Iteration 195, Loss: 5.985066290423475e-07\n",
            "Iteration 196, Loss: 5.818657507284256e-07\n",
            "Iteration 197, Loss: 5.656875553950499e-07\n",
            "Iteration 198, Loss: 5.499591786049475e-07\n",
            "Iteration 199, Loss: 5.346681136030431e-07\n",
            "Iteration 200, Loss: 5.198022013723401e-07\n",
            "Final weights: [0.52846987 0.74306025 1.02846987]\n",
            "Final bias: 0.07153012563858163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k6fAVOGSjO_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}